{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9276abcc-0459-4b6c-82c0-d727697b2910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 10:32:23.027428: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-30 10:32:23.032439: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-30 10:32:23.102978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-30 10:32:24.320355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d68c76-4b19-4e45-b5f2-24f78eb05c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "save_directory    = '/home/ubuntu/tensorflow_datasets/cifar100_grey_16x16_LANCZOS3/'\n",
    "apply_grayscale   = True\n",
    "reduce_dimensions = True\n",
    "normalise_data    = True\n",
    "resize_method     = tf.image.ResizeMethod.LANCZOS3\n",
    "data_size_x       = 16\n",
    "data_size_y       = data_size_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "869c3540-b3f4-4bc4-a8cb-bbaade59321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Summary\n",
    "with open(save_directory + 'configurations.txt', 'w') as f:\n",
    "    f.write(f'save_directory = {save_directory}\\n')\n",
    "    f.write(f'apply_grayscale = {apply_grayscale}\\n')\n",
    "    f.write(f'reduce_dimensions = {reduce_dimensions}\\n')\n",
    "    f.write(f'normalise_data = {normalise_data}\\n')\n",
    "    f.write(f'resize_method = {resize_method}\\n')\n",
    "    f.write(f'data_size_x = {data_size_x}\\n')\n",
    "    f.write(f'data_size_y = {data_size_y}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb3ebc37-4cb0-4c03-b6d6-8985503d3388",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 10:32:35.716054: W external/local_tsl/tsl/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata.google.internal\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset 160.71 MiB (download: 160.71 MiB, generated: 132.03 MiB, total: 292.74 MiB) to /home/ubuntu/tensorflow_datasets/cifar100/3.0.2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28bb69958be0452b85d831a2e6bb3264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06a1dcfae4f48a4a47dc4b3dabc0cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d70507be70e4215b9065dc872092296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1139fabe614bfcbf16727c4c522cc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8ee527b8b5460cb51582ab725f0cd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 10:32:51.059346: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89718c999644b10be36b6aba9b4322f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/ubuntu/tensorflow_datasets/cifar100/3.0.2.incompleteK0ZUCG/cifar100-train.tfrecord*...:   0%| …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2c04caae6be474a90a73d4d616f4d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b19ff68ef974fbda9d232348257a8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /home/ubuntu/tensorflow_datasets/cifar100/3.0.2.incompleteK0ZUCG/cifar100-test.tfrecord*...:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cifar100 downloaded and prepared to /home/ubuntu/tensorflow_datasets/cifar100/3.0.2. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "cifar100 = tfds.builder('cifar100')\n",
    "\n",
    "# Description of dataset\n",
    "assert cifar100.info.features['image'].shape       == (32, 32, 3)\n",
    "assert cifar100.info.features['label'].num_classes == 100\n",
    "assert cifar100.info.splits['train'].num_examples  == 50000\n",
    "assert cifar100.info.splits['test'].num_examples   == 10000\n",
    "\n",
    "# Download and prepare the data\n",
    "cifar100.download_and_prepare()\n",
    "datasets = cifar100.as_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "facec9a3-c171-46f9-9e1a-e86f6aa9a578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and convert to grayscale\n",
    "if apply_grayscale:\n",
    "    for split in ['train', 'test']:\n",
    "        datasets[split] = datasets[split].map(lambda item: {\n",
    "            'image': tf.image.rgb_to_grayscale(item['image']),\n",
    "            'label': item['label']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a5732f-714b-4834-ba5c-dd3d8fd02654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensionality\n",
    "if reduce_dimensions:\n",
    "    for split in ['train', 'test']:\n",
    "        datasets[split] = datasets[split].map(lambda item: {\n",
    "            'image': tf.image.resize(\n",
    "                item['image'],\n",
    "                [data_size_x, data_size_y],\n",
    "                method=resize_method, \n",
    "            ),\n",
    "            'label': item['label']\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e585a743-e85b-49fc-9969-88b20ecb9563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-30 10:33:55.546984: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2024-04-30 10:33:55.548619: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Normalise\n",
    "if normalise_data:\n",
    "    # Take one example from the dataset\n",
    "    for example in datasets['train'].take(1):\n",
    "        image, label = example['image'], example['label']\n",
    "\n",
    "    # Check maximal pixel value and normalise if above 1\n",
    "    max_pixel_value = tf.reduce_max(image).numpy()\n",
    "    if max_pixel_value > 1:    \n",
    "        for split in ['train', 'test']:\n",
    "            datasets[split] = datasets[split].map(lambda item: {\n",
    "                'image': item['image'] / 255,\n",
    "                'label': item['label']\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ac8edb-275f-4c7b-a429-00bdc0053699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the datasets\n",
    "for split in ['train', 'test']:\n",
    "    datasets[split].save(save_directory + split)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
